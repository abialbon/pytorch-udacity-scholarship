{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST MLP Exercise - Revision.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abialbon/pytorch-udacity-scholarship/blob/master/PyTorch/MNIST_MLP_Exercise_Revision.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "mtLlmxEZytmL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# MNIST MLP Exercise Revision\n",
        "\n",
        "Things to do:\n",
        "1. Get data from the MNIST dataset (I will use this so training takes a lesser time)\n",
        "2. Split the data into train, validation and test sets\n",
        "3. Create a neural network with 2 hidden layers\n",
        "4. Train the layer\n",
        "5. Calculate the training loss, validation loss and accuracy in the test set\n",
        "6. Save the model that performs best"
      ]
    },
    {
      "metadata": {
        "id": "v3uXTLsFzZu6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Installing PyTorch"
      ]
    },
    {
      "metadata": {
        "id": "Bxp1NSeOzdJl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "e0b7dc82-3303-4886-95a5-3d68fd1a5c41"
      },
      "cell_type": "code",
      "source": [
        "# http://pytorch.org/\n",
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tcmalloc: large alloc 1073750016 bytes == 0x58906000 @  0x7fdd7fa4c2a4 0x594e17 0x626104 0x51190a 0x4f5277 0x510c78 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f6070 0x510c78 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f6070 0x4f3338 0x510fb0 0x5119bd 0x4f6070\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jByyBU8Dz6ab",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Importing the datasets"
      ]
    },
    {
      "metadata": {
        "id": "8j0LNAB_z5dB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "n_val = 0.2\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = datasets.MNIST('data', train=True, download=True, transform=transform)\n",
        "testset = datasets.MNIST('data', train=False, download=True, transform=transform)\n",
        "\n",
        "total_to_train = len(trainset)\n",
        "idx = np.array([i for i in range(total_to_train)])\n",
        "np.random.shuffle(idx)\n",
        "split = int(np.floor(n_val * total_to_train))\n",
        "val_idx, train_idx = idx[:split], idx[split:]\n",
        "\n",
        "from torch.utils.data import SubsetRandomSampler\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "val_sampler = SubsetRandomSampler(val_idx)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(trainset, batch_size=32, sampler=train_sampler)\n",
        "val_loader = torch.utils.data.DataLoader(trainset, batch_size=32, sampler=val_sampler)\n",
        "test_loader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-7KQVsZc4TWl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Creating the neural network"
      ]
    },
    {
      "metadata": {
        "id": "58CzWPCI4XmF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "e44bbe37-5d0b-4938-98d1-11cc19c275cd"
      },
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(784, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, 32)\n",
        "        self.fc4 = nn.Linear(32, 10)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        \n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 784)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = F.log_softmax(self.fc4(x), dim=1)\n",
        "        return x\n",
        "    \n",
        "\n",
        "model = Classifier()\n",
        "print(model)\n",
        "model.to(device)\n",
        "\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.003)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classifier(\n",
            "  (fc1): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
            "  (fc3): Linear(in_features=128, out_features=32, bias=True)\n",
            "  (fc4): Linear(in_features=32, out_features=10, bias=True)\n",
            "  (dropout): Dropout(p=0.2)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NvVh_9qY7Kil",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training & validating the model"
      ]
    },
    {
      "metadata": {
        "id": "8T8cD9zM7QxD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "outputId": "0f895809-e9c6-4d02-c91b-61b5df841db0"
      },
      "cell_type": "code",
      "source": [
        "epochs = 30\n",
        "min_val_loss = np.Inf\n",
        "\n",
        "for e in range(epochs):\n",
        "    model.train()\n",
        "    \n",
        "    r_train_loss = 0\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        # Reseting the optimizer\n",
        "        optimizer.zero_grad()\n",
        "        # Forward pass\n",
        "        output = model.forward(images)\n",
        "        loss = criterion(output, labels)\n",
        "        r_train_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "    else:\n",
        "        model.eval()\n",
        "        r_val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                output = model.forward(images)\n",
        "                loss = criterion(output, labels)\n",
        "                r_val_loss += loss.item()\n",
        "            else:\n",
        "                print('Epoch: {} ---- Training loss: {:.3f} ---- Val loss: {:.3f}'.format(e+1, r_train_loss/len(train_loader), r_val_loss/len(val_loader)))\n",
        "                if r_val_loss <= min_val_loss:\n",
        "                    min_val_loss = r_val_loss\n",
        "                    model_dict = {\n",
        "                        'n_input': 784,\n",
        "                        'n_hidden': [256, 128, 32],\n",
        "                        'n_output': 10,\n",
        "                        'state_dict': model.state_dict()\n",
        "                    }\n",
        "                    torch.save(model_dict, 'model.pt')\n",
        "                    print('The model parameters have been saved')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1 ---- Training loss: 0.204 ---- Val loss: 0.186\n",
            "The model parameters have been saved\n",
            "Epoch: 2 ---- Training loss: 0.195 ---- Val loss: 0.150\n",
            "The model parameters have been saved\n",
            "Epoch: 3 ---- Training loss: 0.180 ---- Val loss: 0.135\n",
            "The model parameters have been saved\n",
            "Epoch: 4 ---- Training loss: 0.180 ---- Val loss: 0.141\n",
            "Epoch: 5 ---- Training loss: 0.162 ---- Val loss: 0.122\n",
            "The model parameters have been saved\n",
            "Epoch: 6 ---- Training loss: 0.165 ---- Val loss: 0.126\n",
            "Epoch: 7 ---- Training loss: 0.159 ---- Val loss: 0.122\n",
            "The model parameters have been saved\n",
            "Epoch: 8 ---- Training loss: 0.154 ---- Val loss: 0.142\n",
            "Epoch: 9 ---- Training loss: 0.149 ---- Val loss: 0.125\n",
            "Epoch: 10 ---- Training loss: 0.148 ---- Val loss: 0.180\n",
            "Epoch: 11 ---- Training loss: 0.144 ---- Val loss: 0.126\n",
            "Epoch: 12 ---- Training loss: 0.138 ---- Val loss: 0.122\n",
            "The model parameters have been saved\n",
            "Epoch: 13 ---- Training loss: 0.140 ---- Val loss: 0.131\n",
            "Epoch: 14 ---- Training loss: 0.138 ---- Val loss: 0.132\n",
            "Epoch: 15 ---- Training loss: 0.138 ---- Val loss: 0.149\n",
            "Epoch: 16 ---- Training loss: 0.126 ---- Val loss: 0.141\n",
            "Epoch: 17 ---- Training loss: 0.134 ---- Val loss: 0.124\n",
            "Epoch: 18 ---- Training loss: 0.125 ---- Val loss: 0.108\n",
            "The model parameters have been saved\n",
            "Epoch: 19 ---- Training loss: 0.123 ---- Val loss: 0.121\n",
            "Epoch: 20 ---- Training loss: 0.129 ---- Val loss: 0.123\n",
            "Epoch: 21 ---- Training loss: 0.126 ---- Val loss: 0.123\n",
            "Epoch: 22 ---- Training loss: 0.117 ---- Val loss: 0.115\n",
            "Epoch: 23 ---- Training loss: 0.121 ---- Val loss: 0.127\n",
            "Epoch: 24 ---- Training loss: 0.120 ---- Val loss: 0.117\n",
            "Epoch: 25 ---- Training loss: 0.119 ---- Val loss: 0.119\n",
            "Epoch: 26 ---- Training loss: 0.117 ---- Val loss: 0.126\n",
            "Epoch: 27 ---- Training loss: 0.116 ---- Val loss: 0.115\n",
            "Epoch: 28 ---- Training loss: 0.113 ---- Val loss: 0.110\n",
            "Epoch: 29 ---- Training loss: 0.115 ---- Val loss: 0.130\n",
            "Epoch: 30 ---- Training loss: 0.117 ---- Val loss: 0.119\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "g-Y2zhNs_ULl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2d5aaeb4-19bf-485c-d81b-00c424dc1c5f"
      },
      "cell_type": "code",
      "source": [
        "# Calculating the accuracy\n",
        "with torch.no_grad():\n",
        "    acc = 0\n",
        "    model.eval()\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        output = model.forward(images)\n",
        "        ps = torch.exp(output)\n",
        "        _ , y_pred = torch.max(ps, dim=1)\n",
        "        equals = y_pred == labels.view(*y_pred.shape)\n",
        "        equals = equals.type(torch.FloatTensor)\n",
        "        acc += equals.mean().item()\n",
        "    else:\n",
        "        print('The accuracy: {:.2f}%'.format((acc/len(test_loader)*100)))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The accuracy: 96.91%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
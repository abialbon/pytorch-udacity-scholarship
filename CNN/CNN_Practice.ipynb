{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN Practice.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abialbon/pytorch-udacity-scholarship/blob/master/CNN/CNN_Practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "rylARrFLF2Hu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# http://pytorch.org/\n",
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JNu368TpGAS7",
        "colab_type": "code",
        "outputId": "0f87fb15-300e-400e-c538-fa458010033f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "metadata": {
        "id": "SFrBJcVKGhT3",
        "colab_type": "code",
        "outputId": "56cf22a5-f21d-4c46-d698-0a771fa8b1e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "from torch.utils.data import SubsetRandomSampler\n",
        "\n",
        "train_transform = transforms.Compose([transforms.RandomAffine(10),      \n",
        "                                transforms.RandomHorizontalFlip(p=0.5),\n",
        "                                transforms.RandomRotation(10),\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
        "\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                               transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
        "\n",
        "trainset = datasets.CIFAR10('/data', train=True, transform=train_transform, download=True)\n",
        "testset = datasets.CIFAR10('/data', train=False, transform=transform, download=True)\n",
        "\n",
        "val_fraction = 0.5\n",
        "i = int(len(testset) * val_fraction)\n",
        "idx = np.array([x for x in range(len(testset))])\n",
        "np.random.shuffle(idx)\n",
        "\n",
        "val_idx, test_idx = SubsetRandomSampler(idx[:i]), SubsetRandomSampler(idx[i:])\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True)\n",
        "validloader = torch.utils.data.DataLoader(testset, batch_size=32, sampler= val_idx)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=32, sampler= test_idx)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "auB66GwJH1jA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def imshow(tensor):\n",
        "    tensor = (tensor * 0.5) + 0.5\n",
        "    tensor = tensor.numpy().squeeze()\n",
        "    tensor = tensor.transpose(1, 2, 0)\n",
        "    plt.imshow(tensor)\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NeKkNrGdOOoZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ImageClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.c1 = nn.Conv2d(3, 16, 3, padding=1)\n",
        "        self.c2 = nn.Conv2d(16, 32, 3, padding=1)\n",
        "        self.c3 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        \n",
        "        self.max_pool = nn.MaxPool2d(2, 2)\n",
        "        \n",
        "        self.fc1 = nn.Linear(64*4*4, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "        \n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        \n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.max_pool(F.relu(self.c1(x)))\n",
        "        x = self.max_pool(F.relu(self.c2(x)))\n",
        "        x = self.max_pool(F.relu(self.c3(x)))\n",
        "        x = x.view(-1, 64*4*4)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = F.log_softmax(self.fc2(x), dim=1)\n",
        "        return x\n",
        "    \n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zwB25adHcGLF",
        "colab_type": "code",
        "outputId": "0ced1284-0043-41d6-e67f-aa089cb8afc3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "cell_type": "code",
      "source": [
        "model = ImageClassifier()\n",
        "model.to(device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ImageClassifier(\n",
              "  (c1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (c2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (c3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (max_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (fc1): Linear(in_features=1024, out_features=128, bias=True)\n",
              "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
              "  (dropout): Dropout(p=0.2)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "metadata": {
        "id": "lvdANfP1cc_0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "beSx8sjQgRz2",
        "colab_type": "code",
        "outputId": "8d1b5260-95c7-4154-b16d-ae5490dbe1d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        }
      },
      "cell_type": "code",
      "source": [
        "epochs = 20\n",
        "val_min = np.inf\n",
        "\n",
        "for e in range(epochs):\n",
        "    model.train()\n",
        "    \n",
        "    train_loss = 0\n",
        "    \n",
        "    for images, labels in trainloader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        \n",
        "        output = model.forward(images)\n",
        "        loss = criterion(output, labels)\n",
        "        train_loss += loss.item()\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "    else:\n",
        "        val_loss = 0\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for images, labels in validloader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                output = model.forward(images)\n",
        "                loss = criterion(output, labels)\n",
        "                val_loss += loss.item()\n",
        "            else:\n",
        "                avg_val_loss = val_loss/len(validloader)\n",
        "                print('Epoch: {} ---- Train loss: {:.3f} ---- Val loss: {:.3f}'.format(e+1, train_loss/len(trainloader), avg_val_loss))\n",
        "                if avg_val_loss < val_min:\n",
        "                    val_min = avg_val_loss\n",
        "                    torch.save(model.state_dict(), 'model.pth')\n",
        "                    print('Model has been saved successfully!')\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1 ---- Train loss: 1.571 ---- Val loss: 1.301\n",
            "Model has been saved successfully!\n",
            "Epoch: 2 ---- Train loss: 1.273 ---- Val loss: 1.121\n",
            "Model has been saved successfully!\n",
            "Epoch: 3 ---- Train loss: 1.142 ---- Val loss: 1.052\n",
            "Model has been saved successfully!\n",
            "Epoch: 4 ---- Train loss: 1.050 ---- Val loss: 0.919\n",
            "Model has been saved successfully!\n",
            "Epoch: 5 ---- Train loss: 0.994 ---- Val loss: 0.922\n",
            "Epoch: 6 ---- Train loss: 0.952 ---- Val loss: 0.863\n",
            "Model has been saved successfully!\n",
            "Epoch: 7 ---- Train loss: 0.915 ---- Val loss: 0.850\n",
            "Model has been saved successfully!\n",
            "Epoch: 8 ---- Train loss: 0.885 ---- Val loss: 0.829\n",
            "Model has been saved successfully!\n",
            "Epoch: 9 ---- Train loss: 0.860 ---- Val loss: 0.807\n",
            "Model has been saved successfully!\n",
            "Epoch: 10 ---- Train loss: 0.845 ---- Val loss: 0.828\n",
            "Epoch: 11 ---- Train loss: 0.833 ---- Val loss: 0.767\n",
            "Model has been saved successfully!\n",
            "Epoch: 12 ---- Train loss: 0.816 ---- Val loss: 0.831\n",
            "Epoch: 13 ---- Train loss: 0.802 ---- Val loss: 0.778\n",
            "Epoch: 14 ---- Train loss: 0.792 ---- Val loss: 0.750\n",
            "Model has been saved successfully!\n",
            "Epoch: 15 ---- Train loss: 0.778 ---- Val loss: 0.759\n",
            "Epoch: 16 ---- Train loss: 0.770 ---- Val loss: 0.769\n",
            "Epoch: 17 ---- Train loss: 0.763 ---- Val loss: 0.768\n",
            "Epoch: 18 ---- Train loss: 0.758 ---- Val loss: 0.737\n",
            "Model has been saved successfully!\n",
            "Epoch: 19 ---- Train loss: 0.744 ---- Val loss: 0.760\n",
            "Epoch: 20 ---- Train loss: 0.739 ---- Val loss: 0.732\n",
            "Model has been saved successfully!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vh0r-u4SiNnj",
        "colab_type": "code",
        "outputId": "7f9bbf87-8bf7-4ccd-d30d-57fd6871297e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    accuracy = 0\n",
        "    for images, labels in testloader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        output = model.forward(images)\n",
        "        probs = torch.exp(output)\n",
        "        p, top_c = probs.topk(1)\n",
        "        equals = top_c.squeeze() == labels\n",
        "        equals = equals.type(torch.FloatTensor)\n",
        "        accuracy += equals.mean()\n",
        "    print(accuracy/len(testloader) * 100)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(75.1393)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "M-bTot23iBgR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "l = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QyUWa3Ejl-Yg",
        "colab_type": "code",
        "outputId": "ed3ce6e2-3387-401a-b595-51663c97ec62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "cell_type": "code",
      "source": [
        "images, labels = iter(testloader).next()\n",
        "\n",
        "i = images[20]\n",
        "print(labels[20])\n",
        "o = model.forward(i.unsqueeze(0).to(device))\n",
        "p = torch.exp(o)\n",
        "_, top_c = p.topk(1)\n",
        "\n",
        "print(l[top_c.item()])\n",
        "imshow(i)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(9)\n",
            "truck\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPoAAAD5CAYAAAAOeCiTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npAA1dEVJAWroipIC/j8HzkX9dyyLzQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f7d79184a58>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "wZsF7IYhOOc4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "newmodel = ImageClassifier()\n",
        "newmodel.load_state_dict(torch.load('model.pth'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Bj36h3tDPobf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "newmodel.cuda()\n",
        "with torch.no_grad():\n",
        "    accuracy = 0\n",
        "    for images, labels in testloader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        output = newmodel.forward(images)\n",
        "        probs = torch.exp(output)\n",
        "        p, top_c = probs.topk(1)\n",
        "        equals = top_c.squeeze() == labels\n",
        "        equals = equals.type(torch.FloatTensor)\n",
        "        accuracy += equals.mean()\n",
        "    print(accuracy/len(testloader) * 100)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
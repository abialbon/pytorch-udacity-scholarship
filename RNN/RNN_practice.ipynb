{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN practice.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abialbon/pytorch-udacity-scholarship/blob/master/RNN/RNN_practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "adMkMoeiZa-4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# RNN Practice\n",
        "\n",
        "Things to do\n",
        "\n",
        "1. Install PyTorch\n",
        "2. Get the dataset\n",
        "3. Create model wrapper\n",
        "4. Create train function\n",
        "5. Write the function that creates batches\n",
        "6. Write the function that one_hot_encodes\n",
        "7. Write a function that can encode/decode\n",
        "\n",
        "8. Train the model\n",
        "9. Save the model to google drive\n",
        "10. Write a function that can predict given a char\n",
        "11. Write a function that will create text from a starting prime"
      ]
    },
    {
      "metadata": {
        "id": "ECACqUYAZTfA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# http://pytorch.org/\n",
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sAbKlL7AbwKm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#All imports here:\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "# Global variables\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8UfZS4PkarlS",
        "colab_type": "code",
        "outputId": "9f69567f-7070-4e30-ccb0-7d45d6d80d63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/abialbon/deep-learning-v2-pytorch/master/recurrent-neural-networks/char-rnn/data/anna.txt"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-11-28 23:58:06--  https://raw.githubusercontent.com/abialbon/deep-learning-v2-pytorch/master/recurrent-neural-networks/char-rnn/data/anna.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2025486 (1.9M) [text/plain]\n",
            "Saving to: ‘anna.txt.1’\n",
            "\n",
            "\ranna.txt.1            0%[                    ]       0  --.-KB/s               \ranna.txt.1          100%[===================>]   1.93M  10.6MB/s    in 0.2s    \n",
            "\n",
            "2018-11-28 23:58:06 (10.6 MB/s) - ‘anna.txt.1’ saved [2025486/2025486]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "P8wBwN0WenxN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with open('anna.txt', 'r') as f:\n",
        "    data = f.read()\n",
        "    tokens = set(data)\n",
        "    n_tokens = len(set(data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4qE46THyatIO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Model class\n",
        "class CharRNN(nn.Module):\n",
        "    def __init__(self, tokens, n_hidden_layers=512, n_layers=2, drop_prob=0.5):\n",
        "        super().__init__()\n",
        "        self.tokens = tokens\n",
        "        self.n_tokens = len(tokens)\n",
        "        self.char2int = {ch:i for i, ch in enumerate(self.tokens)}\n",
        "        self.int2char = {i:ch for ch, i in self.char2int.items()}\n",
        "        self.hidden_layers = n_hidden_layers\n",
        "        self.n_layers = n_layers\n",
        "        # Layers\n",
        "        self.lstm = nn.LSTM(self.n_tokens, n_hidden_layers, n_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(n_hidden_layers, self.n_tokens)\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        \n",
        "    \n",
        "    def forward(self, x, h):\n",
        "        x, h = self.lstm(x, h)\n",
        "        x = self.dropout(x)\n",
        "        x = x.contiguous().view(-1, self.hidden_layers)\n",
        "        x = self.fc(x)\n",
        "        return x, h\n",
        "    \n",
        "    \n",
        "    def encode(self, item, dict):\n",
        "        return dict[item]\n",
        "    \n",
        "    \n",
        "    \n",
        "    def init_hidden_state(self, batch_size):\n",
        "        weights = next(self.parameters()).data\n",
        "        return (weights.new(self.n_layers, batch_size, self.hidden_layers).zero_().to(device),\n",
        "               weights.new(self.n_layers, batch_size, self.hidden_layers).zero_().to(device))\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C4jPw69Ou7gV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def one_hot_encode(arr, n_labels):\n",
        "    \"\"\"\n",
        "    Generates one hot encoded array with n_labels number of labels\n",
        "    \n",
        "    Arguments:\n",
        "    ----------\n",
        "    arr: an array of shape x, y\n",
        "    n_labels: one hot labels to encode\n",
        "    \n",
        "    Output:\n",
        "    -------\n",
        "    One hot encoded array of size (x, y, n_labels)\n",
        "    \"\"\"\n",
        "    i = np.multiply(*arr.shape)\n",
        "    one_hot = np.zeros((i, n_labels))\n",
        "    one_hot[np.arange(i), arr.flatten()] = 1\n",
        "    one_hot = one_hot.reshape(*arr.shape, n_labels)\n",
        "    return one_hot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RthLbn5ImcVE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generate_batches(arr, b, s):\n",
        "    \"\"\"\n",
        "    Generates batches of x, y with shapes b x s with y shifted one to the right\n",
        "    \n",
        "    Argumants:\n",
        "    ----------\n",
        "    arr: a flat array of size x, y\n",
        "    b: batch size\n",
        "    s: sequence length\n",
        "    \n",
        "    Outputs:\n",
        "    generator tuple(x, y) each with a dimension of (b x s)\n",
        "    \"\"\"\n",
        "    n = len(arr) // (b * s)\n",
        "    arr = arr[:n*b*s]\n",
        "    arr = arr.reshape(b, -1)\n",
        "    for step in range(0, n*s, s):\n",
        "        x = arr[:, step:step+s]\n",
        "        y = np.zeros_like(x)\n",
        "        try:\n",
        "            y[:,:-1], y[:,-1] = x[:, 1:], arr[:, step+s]\n",
        "        except:\n",
        "            y[:,:-1], y[:,-1] = x[:, 1:], arr[:, 0]\n",
        "        yield x, y\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kLvJKNrye5VO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(model, data, epochs=5, batch_size=128, seq_length=100, val_fraction=0.1, lr=0.001):\n",
        "    \n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    \n",
        "    model.to(device)\n",
        "    model.train()\n",
        "    \n",
        "    encoded_data = np.array([model.encode(i, model.char2int) for i in data])\n",
        "    idx = int(len(data) * (1-val_fraction))\n",
        "    trainset, valset = encoded_data[:idx], encoded_data[idx:]\n",
        "   \n",
        "    \n",
        "    for e in range(1, epochs+1):\n",
        "        train_loss = 0\n",
        "        t_counter = 0\n",
        "        h = model.init_hidden_state(batch_size)\n",
        "\n",
        "        for inputs, target in generate_batches(trainset, batch_size, seq_length):\n",
        "            inputs = one_hot_encode(inputs, model.n_tokens)\n",
        "            inputs = torch.from_numpy(inputs).type(torch.FloatTensor).to(device)\n",
        "            target = torch.from_numpy(target).to(device)\n",
        "            \n",
        "            h = tuple([each.data for each in h])\n",
        "            \n",
        "            output, h = model.forward(inputs, h)\n",
        "            optimizer.zero_grad()\n",
        "            loss = criterion(output, target.view(batch_size*seq_length))\n",
        "            train_loss += loss.item()\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
        "            optimizer.step()\n",
        "            t_counter += 1\n",
        "            \n",
        "        else:\n",
        "            model.eval()\n",
        "            v_h = model.init_hidden_state(batch_size)\n",
        "            v_loss = 0\n",
        "            v_counter = 0\n",
        "            with torch.no_grad():\n",
        "                for v_inputs, v_target in generate_batches(valset, batch_size, seq_length):\n",
        "                    v_inputs = one_hot_encode(v_inputs, model.n_tokens)\n",
        "                    v_inputs = torch.from_numpy(v_inputs).type(torch.FloatTensor).to(device)\n",
        "                    v_target = torch.from_numpy(v_target).to(device)\n",
        "\n",
        "                    v_h = tuple([each.data for each in h])\n",
        "\n",
        "                    output, v_h = model.forward(v_inputs, v_h)\n",
        "                    loss = criterion(output, v_target.view(batch_size * seq_length))\n",
        "                    v_loss += loss.item()\n",
        "                    v_counter += 1\n",
        "                    \n",
        "                else:\n",
        "                    model.train()\n",
        "                    print('Epoch: {} ---- Train loss: {:.3f} ---- Val loss: {:.3f}'.format(e, train_loss/t_counter, v_loss/v_counter))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zA9TDf1pmAqA",
        "colab_type": "code",
        "outputId": "119687d4-ee5d-46bf-a1c4-52c904d92d68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "net = CharRNN(tokens)\n",
        "net.to(device)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CharRNN(\n",
              "  (lstm): LSTM(83, 512, num_layers=2, batch_first=True)\n",
              "  (fc): Linear(in_features=512, out_features=83, bias=True)\n",
              "  (dropout): Dropout(p=0.5)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "metadata": {
        "id": "qr5FPzTXwr-F",
        "colab_type": "code",
        "outputId": "f883017c-9f13-4759-dc18-b00d8ee3c6e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        }
      },
      "cell_type": "code",
      "source": [
        "train(net, data, epochs=5)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1786700\n",
            "Epoch: 1 ---- Train loss: 1.055 ---- Val loss: 1.470\n",
            "Epoch: 2 ---- Train loss: 1.039 ---- Val loss: 1.482\n",
            "Epoch: 3 ---- Train loss: 1.026 ---- Val loss: 1.484\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-77-e627c3d9089a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-68-c93f16e1f60b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, data, epochs, batch_size, seq_length, val_fraction, lr)\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mt_counter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mparam_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0mtotal_norm\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mparam_norm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_norm\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "s6XeKeMT1r4x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def predict_next_char(model, char, h, top_k):\n",
        "    h = tuple([each.data for each in h])\n",
        "    encoded = np.array([[model.encode(i, model.char2int) for i in char]])\n",
        "    one_hot_encoded = one_hot_encode(encoded, model.n_tokens)\n",
        "    \n",
        "    x = torch.from_numpy(one_hot_encoded).type(torch.FloatTensor).to(device)\n",
        "    \n",
        "    output, h = model.forward(x, h)\n",
        "    probs = F.softmax(output, dim=1)\n",
        "    p, top_c = probs.topk(top_k)\n",
        "    \n",
        "    p = p.detach().cpu().numpy().squeeze()\n",
        "    top_c = top_c.detach().cpu().numpy().squeeze()\n",
        "    if top_k is not 1:\n",
        "        char = np.random.choice(top_c, p=p/p.sum())\n",
        "    else:\n",
        "        char = top_c\n",
        "    char = model.encode(int(char), model.int2char)\n",
        "    \n",
        "    return char, h "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qPZc4ZN8s5t2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sample(model, size=200, prime='The', top_k=5):\n",
        "    chars = [ch for ch in prime]\n",
        "    h = model.init_hidden_state(1)\n",
        "    for i in prime:\n",
        "        char, h = predict_next_char(model, i, h, top_k)\n",
        "    chars.append(char)\n",
        "    \n",
        "    for ii in range(size):\n",
        "        char, h = predict_next_char(model, chars[-1], h, top_k)\n",
        "        chars.append(char)\n",
        "        \n",
        "    return ''.join(chars)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9urzmJNkuE8Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "f17a062f-b901-4711-c94a-4f9218e703e1"
      },
      "cell_type": "code",
      "source": [
        "print(sample(net, 1000, 'Anna', 5))"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Anna Arkadyevna, who\n",
            "was seemed an artly with his heart to her, and she would have tried\n",
            "improve of her and he could not have business with a slyes is a small\n",
            "point into the separate attractive. She was not that as the more did\n",
            "not range her a long wife. She stopped and took up a concealive also\n",
            "young, but as shuglish acquaintances.\n",
            "\n",
            "\"I wonder as you're a belichtic,\" said he. \"Is, so that there's a measureful the\n",
            "staying from himself. And she's a light, yes, she does not let,\"\n",
            "she said, \"I haven't forgiven, and I've bold the stillness of sufferings\n",
            "and the souncts of all their position, but if I'rriel and saving\n",
            "to me, though a bang haddblows.\"\n",
            "\n",
            "\"Oh, we all say now singing as with a geal of destroum of dead.\"\n",
            "\n",
            "He gazed at Varenka.\n",
            "\n",
            "\"With yous went on, I am doing as though I shall come to the lows that's near\n",
            "him.\n",
            "... and I don't know! Beed her too,\" she said,\n",
            "walking into telling her. Anna was standing before, but therefore,\n",
            "and a screng about the walozed waiter, which he saw nothing, and s\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}